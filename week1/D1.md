## 2025-11-18: 《图解Transformer》笔记

### 1. 核心问题 (The "Why")

* **Transformer 解决的核心问题是什么？**
    <!-- * (提示：写下你对 RNN/LSTM 缺点的理解，比如“无法并行计算”、“长距离依赖”问题。) -->
    * 在序列建模（如 NLP、CV）中，同时实现「全局依赖捕捉」与「并行计算」，突破传统模型（RNN/LSTM）的串行瓶颈和长距离依赖弱化问题
    * Transformer 用「自注意力 + 多头机制 + 位置编码」，同时实现了「全局依赖捕捉、多维度关联建模、并行计算」，成为深度学习的 “通用序列建模框架”
### 2. 宏观架构 (The "Map")

* **Transformer 的“形状”是怎样的？**
    <!-- * (提示：用你自己的话描述 Encoder 和 Decoder 各自是干嘛的。) -->
    * Transformer 的 “形状” 是 “输入→编码理解→解码生成→输出” 的串联架构，核心由 “N 个 Encoder 层堆叠成的编码器块” 和 “N 个 Decoder 层堆叠成的解码器块” 组成，中间通过交叉注意力传递信息，全程带着 “位置信息” 做并行计算。
    <!-- * (例如：Encoder 负责“理解”输入句子，Decoder 负责“生成”输出句子。) -->
    * Encoder（编码器）：负责 “读懂输入”—— 把原始输入（比如中文句子、图像碎片）变成结构化的 “全局语义特征”（相当于把输入的 “信息原料” 提炼成 “知识精华”），不直接输出结果，只做 “理解” 工作。
    * Decoder（解码器）：负责 “生成输出”—— 基于 Encoder 提炼的 “知识精华”，再参考自己已经生成的内容（比如翻译时已经写好的英文词），逐句 / 逐 token 生成符合逻辑、不跑偏的输出（比如英文句子、描述文字）。
    * (尝试画一个 ASCII 图： `[输入] -> [N个Encoder层] -> [N个Decoder层] -> [输出]` )

```text
    # 第一步：输入预处理（补全顺序信息）
[原始输入序列]  # 如：中文句子“我爱机器学习”、图像拆成的16×16补丁
↓
[嵌入层(Embedding) + 位置编码(Positional Encoding)]  # 给每个token加“语义+顺序”信息
↓  # 输出维度：(序列长度n, 嵌入维度d_model)，比如(10, 512)

# 第二步：编码理解（N层堆叠，默认N=6）
[Encoder层1] → [Encoder层2] → ... → [Encoder层N]
  每层内部：多头自注意力 → 残差连接+层归一化 → 前馈神经网络 → 残差连接+层归一化
↓  # 输出：(n, d_model)，结构化的全局语义特征（“知识精华”）

# 第三步：解码生成（N层堆叠，默认N=6，与Encoder层一一对应）
[Decoder层1] → [Decoder层2] → ... → [Decoder层N]
  每层内部：掩码多头自注意力（不偷看未来）→ 残差连接+层归一化 →
            交叉注意力（对接Encoder特征）→ 残差连接+层归一化 →
            前馈神经网络 → 残差连接+层归一化
↓  # 输出：(生成序列长度m, d_model)，比如翻译后英文句子长度m=12

# 第四步：输出最终结果
[线性层(Linear) + Softmax]  # 把特征映射到词汇表，选概率最高的token
↓
[最终输出序列]  # 如：英文句子“I love machine learning”
```
    

### 3. 核心机制 (The "How" - 概念版)

* **Self-Attention (自注意力) 是什么？**
    <!-- * (提示：**不要**写 Q, K, V！**不要**写公式！) -->
  * 让序列中每个词（或元素），在处理时都能 “回头看” 整个序列，自动找到和自己最相关的其他词，从而更准确地理解自身含义
  * 它解决的核心问题是：模型不再 “孤立看待” 每个词，而是能捕捉词与词之间的 “隐藏关联”—— 不管这些词在序列中隔多远。
  * 核心特点（不用公式也能懂）
    * 无距离限制：不管两个词隔多少个词（比如第 1 个词和第 100 个词），都能直接建立关联；
    * 自动加权：不是平等看待所有词，而是自动判断 “谁和我最相关”，给相关度高的词 “更高权重”；
    * 并行处理：所有词可以同时 “回头看” 找关联，不用像 RNN 那样按顺序排队处理。
    <!-- * (提示：写下那个著名的 "it" 例子。例如：“Self-Attention 是一种让模型在处理 'it' 这个词时，能回头去‘看’它到底是指 'animal' 还是 'street' 的技术。”) -->
  * 看两个句子
```text
1."The animal didn't cross the street because it was too tired."（这只动物没过马路，因为它太累了。）
2."The animal didn't cross the street because it was too wide."（这只动物没过马路，因为它太宽了。）
```

* Self-Attention 就是让模型在处理 "it" 这个词时，能自动 “扫描” 整个句子，判断出：
    * 句 1 中 "it" 和 "the animal" 的关联最强，所以理解为 “动物”；
    * 句 2 中 "it" 和 "the street" 的关联最强，所以理解为 “马路”。
    


### 4. B 计划“英文路标”(The "Glossary")

<!-- * (提示：这是**最重要的**部分。把你今天“中英对照”时学到的**核心英文黑话**记录下来。这会**极大**帮助你后续读论文和代码。) -->
* 编码器: `Encoder`
* 解码器: `Decoder`
* 自注意力: `Self-Attention`
* 多头注意力: `Multi-Head Attention`
* 位置编码: `Positional Encoding`
<!-- * (你每看到一个新“黑话”，就往这个列表里加。) -->

### 5. 疑问 (My Questions)

<!-- * (提示：把所有你**今天看不懂**的东西列在这里。这是你 Day 4/5 要攻克的“堡垒”。) -->
* *疑问1：* “自注意力”内部到底是怎么算的？
    * 通过给每个元素分配 Q（查询）、K（键）、V（值），计算 Q 与所有 K 的相似度得到权重，经 Softmax 归一化得到概率权重，再用权重对所有 V 加权求和，生成融合全局上下文的新特征。
* *疑问2：* 什么是 Q, K, V？
    * Q（查询）是当前元素“要找什么”的需求载体，K（键）是所有元素“是什么”的索引标识，V（值）是所有元素“有什么核心信息”的内容载体，三者配合实现“按需匹配、按需取信息”的关联计算。
* *疑问3：* 为什么需要“多头”？
    * 不把 “找关联” 的任务交给一个 “单一视角”，而是用 “多个视角” 一起找，最后综合所有视角的信息 —— 让每个词的理解更全面、不偏科
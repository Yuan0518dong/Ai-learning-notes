# NanoGPT for Code: A Minimalist Code Completion Model

![Status](https://img.shields.io/badge/Status-Prototype-green)
![Topic](https://img.shields.io/badge/Topic-LLM%20Training-blue)

è¿™æ˜¯ä¸€ä¸ªåŸºäº Transformer (Decoder-only) æ¶æ„çš„è¿·ä½ ç”Ÿæˆå¼æ¨¡å‹ï¼Œä¸“é—¨ä»é›¶å¼€å§‹è®­ç»ƒç”¨äº **Python ä»£ç è¡¥å…¨** ä»»åŠ¡ã€‚
è¯¥é¡¹ç›®æ˜¯ **Week 2 AI4SE (AI for Software Engineering)** çš„æ ¸å¿ƒå·¥ç¨‹å®è·µã€‚

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„ (Architecture)
æœ¬é¡¹ç›®**ä¸ä¾èµ–**ä»»ä½•é«˜çº§å°è£…åº“ (å¦‚ HuggingFace Trainer)ï¼Œçº¯æ‰‹å·¥å®ç°äº† GPT çš„æ ¸å¿ƒç»„ä»¶ï¼š
* **Causal Masking:** å®ç°äº†ä¸‹ä¸‰è§’æ©ç ï¼Œç¡®ä¿æ¨¡å‹ä¸¥æ ¼éµå®ˆå› æœæ¨ç†ï¼ˆä¸å·çœ‹æœªæ¥ï¼‰ã€‚
* **Transformer Block:** å®ç°äº† Pre-Norm ç»“æ„çš„ Attention + FeedForward å †å ã€‚
* **Positional Embedding:** ä½¿ç”¨äº†å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„
```text
Transformer_Learning/
â”œâ”€â”€ data/               # è®­ç»ƒæ•°æ® (Python æºç )
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ attention.py    # Multi-Head Attention + Mask æ‰‹å†™å®ç°
â”‚   â”œâ”€â”€ gpt.py          # GPT æ¨¡å‹ä¸»æ¶æ„ (Block, FeedForward)
â”œâ”€â”€ train.py            # è®­ç»ƒè„šæœ¬ (Training Loop)
â”œâ”€â”€ generate.py         # æ¨ç†è„šæœ¬ (Token Sampling)
â””â”€â”€ test_playground.py  # å•å…ƒæµ‹è¯• (Unit Tests)
```
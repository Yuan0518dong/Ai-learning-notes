import torch
import torch.nn as nn
from torch.nn import functional as F
import os

from model.gpt import GPT

# ==========================================
# 1. é…ç½®å‚æ•° (Hyperparameters)
# ==========================================
batch_size = 32  # ä¸€æ¬¡çœ‹å¤šå°‘ä¸ªç‰‡æ®µ
block_size = 64  # ä¸Šä¸‹æ–‡é•¿åº¦ (ä¸€æ¬¡çœ‹å¤šå°‘ä¸ªå­—ç¬¦)
max_iters = 1000  # è®­ç»ƒå¤šå°‘æ­¥
learning_rate = 3e-4  # å­¦ä¹ ç‡
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_interval = 200  # æ¯å¤šå°‘æ­¥è¯„ä¼°ä¸€æ¬¡
n_embd = 384  # åµŒå…¥ç»´åº¦ (d_model)
n_head = 6  # æ³¨æ„åŠ›å¤´æ•°
n_layer = 4  # Block å±‚æ•°
dropout = 0.2

print(f"ğŸ”¥ æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device}")

# ==========================================
# 2. å‡†å¤‡æ•°æ® (Data Pipeline)
# ==========================================
try:
    with open('data/input.txt', 'r', encoding='utf-8') as f:
        text = f.read()
    print(f"ğŸ“š æˆåŠŸåŠ è½½æ•°æ®! é•¿åº¦: {len(text)} å­—ç¬¦")
except FileNotFoundError:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° data/input.txtã€‚è¯·ç¡®ä¿ä½ æ˜¨å¤©å®Œæˆäº†æ•°æ®å‡†å¤‡ä»»åŠ¡ï¼")
    exit()

# --- æ„å»ºç®€å•çš„å­—ç¬¦çº§ Tokenizer ---
chars = sorted(list(set(text)))
vocab_size = len(chars)
print(f"ğŸ”¤ è¯è¡¨å¤§å°: {vocab_size}")
print(f"ğŸ”¤ è¯è¡¨å†…å®¹ (éƒ¨åˆ†): {''.join(chars[:20])}...")

# å»ºç«‹æ˜ å°„è¡¨
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}
encode = lambda s: [stoi[c] for c in s]  # æ–‡æœ¬ -> æ•°å­—
decode = lambda l: ''.join([itos[i] for i in l])  # æ•°å­— -> æ–‡æœ¬

# åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†
data = torch.tensor(encode(text), dtype=torch.long)
n = int(0.9 * len(data))  # 90% è®­ç»ƒ
train_data = data[:n]
val_data = data[n:]


# --- è·å– Batch çš„å‡½æ•° ---
def get_batch(split):
    data_source = train_data if split == 'train' else val_data
    # éšæœºé€‰ batch_size ä¸ªèµ·å§‹ç‚¹
    ix = torch.randint(len(data_source) - block_size, (batch_size,))
    # æå–è¾“å…¥ x å’Œ ç›®æ ‡ y (y å°±æ˜¯ x å‘åç§»ä¸€ä½)
    x = torch.stack([data_source[i:i + block_size] for i in ix])
    y = torch.stack([data_source[i + 1:i + block_size + 1] for i in ix])
    return x.to(device), y.to(device)


# ==========================================
# 3. åˆå§‹åŒ–æ¨¡å‹
# ==========================================
model = GPT(vocab_size=vocab_size,
            d_model=n_embd,
            n_layer=n_layer,
            n_head=n_head,
            max_len=block_size,  # æ³¨æ„ï¼šè¿™é‡Œè¦å’Œ block_size å¯¹é½
            dropout=dropout)
model = model.to(device)
print(f"ğŸ¤– æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M")

# ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

# ==========================================
# 4. è®­ç»ƒå¾ªç¯ (The Loop)
# ==========================================
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")

for iter in range(max_iters):

    # --- è¯„ä¼°é˜¶æ®µ (ä¸æ›´æ–°å‚æ•°) ---
    if iter % eval_interval == 0 or iter == max_iters - 1:
        model.eval()
        with torch.no_grad():
            # ç®€å•ä¼°ç®—ä¸€ä¸‹å½“å‰çš„ Loss
            x_val, y_val = get_batch('val')
            logits = model(x_val)
            # è®¡ç®— CrossEntropyLoss
            # logits: [B, T, vocab_size] -> [B*T, vocab_size]
            # targets: [B, T] -> [B*T]
            loss = F.cross_entropy(logits.view(-1, vocab_size), y_val.view(-1))
            print(f"Step {iter}: Val Loss = {loss.item():.4f}")
        model.train()  # åˆ‡å›è®­ç»ƒæ¨¡å¼

    # --- è®­ç»ƒé˜¶æ®µ ---
    # 1. æ‹¿æ•°æ®
    xb, yb = get_batch('train')

    # 2. å‰å‘ä¼ æ’­
    logits = model(xb)

    # 3. ç®— Loss
    loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))

    # 4. åå‘ä¼ æ’­ (ä¸‰æ¿æ–§)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# ==========================================
# 5. ä¿å­˜æ¨¡å‹
# ==========================================
torch.save(model.state_dict(), 'model/nano_gpt_code.pth')
print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° model/nano_gpt_code.pth")

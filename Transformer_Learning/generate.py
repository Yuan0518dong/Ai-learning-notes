import torch
import torch.nn.functional as F
from model.gpt import GPT

# ==========================================
# 1. é…ç½® & å‡†å¤‡ (å¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€è‡´)
# ==========================================
device = 'cuda' if torch.cuda.is_available() else 'cpu'
data_path = 'data/input.txt'
model_path = 'model/nano_gpt_code.pth'

# é‡æ–°æ„å»º Tokenizer (ä¸ºäº†ç¡®ä¿å’Œè®­ç»ƒæ—¶æ˜ å°„è¡¨ä¸€è‡´ï¼Œæˆ‘ä»¬é‡è¯»ä¸€éæ•°æ®)
# (æ³¨ï¼šæ›´å·¥ç¨‹åŒ–çš„åšæ³•æ˜¯æŠŠ stoi/itos ä¿å­˜æˆ pkl æ–‡ä»¶ï¼Œä½†è¿™é‡Œç›´æ¥è¯»æ–‡ä»¶æ›´ç¨³å¦¥)
with open(data_path, 'r', encoding='utf-8') as f:
    text = f.read()

chars = sorted(list(set(text)))
vocab_size = len(chars)
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}
encode = lambda s: [stoi[c] for c in s]
decode = lambda l: ''.join([itos[i] for i in l])

print(f"ğŸ”¥ è®¾å¤‡: {device}")
print(f"ğŸ”¤ è¯è¡¨é‡æ„å®Œæˆï¼Œå¤§å°: {vocab_size}")

# ==========================================
# 2. åŠ è½½æ¨¡å‹
# ==========================================
# âš ï¸ å‚æ•°å¿…é¡»å’Œ train.py é‡Œä¸€æ¨¡ä¸€æ ·ï¼
n_embd = 384
n_head = 6
n_layer = 4
block_size = 64
dropout = 0.2

model = GPT(vocab_size=vocab_size,
            d_model=n_embd,
            n_layer=n_layer,
            n_head=n_head,
            max_len=block_size,
            dropout=dropout)

# åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
print(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡: {model_path} ...")
model.load_state_dict(torch.load(model_path, map_location=device))
model.to(device)
model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (å…³é—­ Dropout)
print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å‡†å¤‡ç”Ÿæˆä»£ç ...")


# ==========================================
# 3. å®šä¹‰ç”Ÿæˆå‡½æ•°
# ==========================================
def generate_code(start_text, max_new_tokens=200):
    # æŠŠæ–‡æœ¬å˜æˆ tensor
    context_idxs = encode(start_text)
    idx = torch.tensor(context_idxs, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]

    print(f"\n{'=' * 20} ç”Ÿæˆç»“æœ {'=' * 20}")
    print(f"ğŸŸ¢ èµ·å§‹æç¤º: {start_text}")
    print(f"ğŸ¤– AIç»­å†™:\n")

    # é€æ­¥ç”Ÿæˆ
    for _ in range(max_new_tokens):
        # æˆªæ–­ contextï¼Œç¡®ä¿ä¸è¶…è¿‡ block_size
        idx_cond = idx[:, -block_size:]

        # å‰å‘ä¼ æ’­
        logits = model(idx_cond)

        # åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹
        logits = logits[:, -1, :]  # [1, vocab_size]

        # ç®—æ¦‚ç‡
        probs = F.softmax(logits, dim=-1)

        # é‡‡æ · (Multinomial Sampling) - å¢åŠ å¤šæ ·æ€§
        idx_next = torch.multinomial(probs, num_samples=1)  # [1, 1]

        # æ‹¼æ¥åˆ°ç»“æœé‡Œ
        idx = torch.cat((idx, idx_next), dim=1)

        # å®æ—¶æ‰“å°å‡ºä¸€ä¸ªå­—ç¬¦ (æ›´æœ‰æ„Ÿè§‰)
        char = decode([idx_next.item()])
        print(char, end='', flush=True)

    print(f"\n\n{'=' * 20} ç»“æŸ {'=' * 20}")


# ==========================================
# 4. ç©è€æ—¶é—´ï¼
# ==========================================
# Case 1: è®©ä»–å†™ä¸ª import
generate_code("import ", max_new_tokens=100)

# Case 2: è®©ä»–å®šä¹‰ä¸ªå‡½æ•°
generate_code("def get_url(url):", max_new_tokens=200)

# Case 3: è®©ä»–å†™ä¸ªç±»
generate_code("class Session:", max_new_tokens=200)
import torch
import torch.nn as nn
from torch.nn.functional import dropout
import math
from model.attention import *


# 1. å·¥å…·å‡½æ•°ï¼šå› æœæ©ç  (Causal Mask)
#    è¿™æ˜¯ Decoder-only æ¶æ„çš„æ ¸å¿ƒï¼Œç¡®ä¿æ¨¡å‹åœ¨å†™ä»£ç æ—¶ä¸èƒ½å·çœ‹åé¢
def create_causal_mask(seq_len):
    """
        ç”Ÿæˆä¸‹ä¸‰è§’æ©ç ã€‚
        å½¢çŠ¶: [seq_len, seq_len]
        1 0 0
        1 1 0
        1 1 1
        """
    mask = torch.ones(seq_len, seq_len)
    mask = torch.tril(mask)

    return mask.bool()

# 2. ä½ç½®ç¼–ç  (Positional Embedding)
#    ä»£ç å¯¹é¡ºåºæåº¦æ•æ„Ÿï¼Œdef å¿…é¡»åœ¨ return å‰é¢
class PositionalEmbedding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        # 1. é€ ä¸€ä¸ªâ€œä½ç½®å­—å…¸â€
        # nn.Embedding æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªå¤§çŸ©é˜µï¼Œå½¢çŠ¶æ˜¯ [max_len, d_model]
        # æ¯”å¦‚ [5000, 512]
        # ç¬¬ 0 è¡Œå­˜çš„æ˜¯â€œä½ç½®0â€çš„ä¸“å±ç‰¹å¾ï¼Œç¬¬ 1 è¡Œå­˜çš„æ˜¯â€œä½ç½®1â€çš„...
        # å…³é”®ç‚¹ï¼šè¿™äº›ç‰¹å¾æ˜¯â€œå¯å­¦ä¹ çš„â€(Learnable) æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šè‡ªå·±å­¦ä¼šâ€œä½ç½®0â€é•¿ä»€ä¹ˆæ ·æœ€å¥½
        self.pe = nn.Embedding(max_len, d_model)

    def forward(self, x):
        # x æ˜¯è¾“å…¥çš„ token åºåˆ—ï¼Œå‡è®¾å½¢çŠ¶æ˜¯ [Batch=2, Seq_Len=10]
        batch_size,seq_len = x.size()

        # 2. ç”Ÿæˆä½ç½®å·ç‰Œ
        # ç”¨ torch.arange ç”Ÿæˆä¸€ä¸ªåºåˆ—: [0, 1, 2, ..., 9]
        # .unsqueeze(0) æ˜¯ä¸ºäº†æŠŠå½¢çŠ¶å˜æˆ [1, 10]ï¼Œæ–¹ä¾¿åé¢å’Œ Batch ç»´åº¦è‡ªåŠ¨å¯¹é½
        position = torch.arange(0, seq_len, device=x.device).unsqueeze(0)

        # 3. æŸ¥è¡¨å¹¶è¿”å›
        # æ‹¿ç€ [0, 1, 2...] å» self.pe è¿™ä¸ªå¤§è¡¨é‡ŒæŸ¥ï¼Œè¿”å›å¯¹åº”çš„å‘é‡
        return self.pe(position)


class FeedForward(nn.Module):
    """
        GPT çš„'è‚Œè‚‰'ï¼šè´Ÿè´£è®°å¿†å’Œéçº¿æ€§å˜æ¢
        ç»“æ„ï¼šLinear -> GELU -> Linear
        """
    def __init__(self, d_model, expansion_factor = 4, dropout = 0.1):
        super().__init__()
        # expansion_factor é»˜è®¤æ˜¯ 4ï¼Œå³ä¸­é—´å±‚ç»´åº¦æ˜¯ 4 å€
        d_ff = d_model * expansion_factor
        self.net = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),# GPT å…³é”®ç»†èŠ‚ï¼šä½¿ç”¨ GELU è€Œé ReLU
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)

class Block(nn.Module):
    """
        ä¸€ä¸ªæ ‡å‡†çš„ GPT Decoder Block
        ç»“æ„ï¼šInput -> LN -> Attn -> Add -> LN -> FFN -> Add
        ç‰¹æ€§ï¼šPre-Norm (å±‚å½’ä¸€åŒ–åœ¨å­å±‚ä¹‹å‰)
        """
    def __init__(self, d_model, n_head, dropout = 0.1):
        super().__init__()
        # 1. æ ¸å¿ƒç»„ä»¶
        self.attention = MultiHeadAttention(d_model, n_head)
        self.ff = FeedForward(d_model)

        # 2. å½’ä¸€åŒ–å±‚ (LayerNorm)
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)

        # 3. Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x,mask = None):
        # === Part 1: Attention ===
        # Pre-Norm ç»“æ„: å…ˆ LNï¼Œå† Attention
        # æ®‹å·®è¿æ¥ (Residual): x = x + sublayer(LN(x))
        x_norm= self.ln1(x)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¼ å…¥ x, x, x æ˜¯å› ä¸ºå®ƒæ˜¯ Self-Attention
        # Decoder è®­ç»ƒæ—¶å¿…é¡»è¦æœ‰ mask (ä¸‹ä¸‰è§’æ©ç )ï¼Œä¸è¿‡ä»Šå¤©å…ˆä¼  None è·‘é€šç»´åº¦å³å¯
        atten_out = self.attention(x_norm,mask)
        x = x + self.dropout(atten_out)

        # === Part 2: FeedForward ===
        x_norm = self.ln2(x)
        ff_out = self.ff(x_norm)
        x = x + self.dropout(ff_out)

        return x

class GPT(nn.Module):

    #æˆ‘ä»¬éœ€è¦å®šä¹‰æ•´ä¸ªç½‘ç»œçš„éª¨æ¶ï¼šEmbedding -> Blocks -> Final Norm -> Output Head

    def __init__(self, vocab_size, d_model, n_layer, n_head, max_len=1024):
        super().__init__()
        # === 1. é›¶ä»¶å‡†å¤‡ï¼šå…¥å£ ===
        # è¯åµŒå…¥ï¼šæŠŠ "101" è¿™ç§æ•°å­—å˜æˆä¸€ä¸ªå‘é‡ [0.1, -0.5, ...] (ä»£è¡¨"è¯­ä¹‰")
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        # ä½ç½®åµŒå…¥ï¼šæŠŠ "ç¬¬0ä¸ªä½ç½®" å˜æˆä¸€ä¸ªå‘é‡ (ä»£è¡¨"é¡ºåº")
        # ä½ çš„ PositionalEmbedding ç±»å°±åœ¨è¿™é‡Œè¢«å®ä¾‹åŒ–
        self.position_embedding = PositionalEmbedding(d_model, max_len)

        # === 2. æ ¸å¿ƒå¼•æ“ï¼šBlock å †å  ===
        # æ·±åº¦å­¦ä¹ ä¹‹æ‰€ä»¥å«â€œæ·±åº¦â€ï¼Œå°±æ˜¯å› ä¸ºè¿™é‡Œå±‚æ•°å¤š
        # æˆ‘ä»¬ç”¨ ModuleList åƒè£…å­å¼¹ä¸€æ ·ï¼Œè£…å…¥ n_layer ä¸ª Block
        # æ¯ä¸ª Block éƒ½èƒ½æå–æ›´é«˜çº§çš„ç‰¹å¾ï¼ˆè¯­æ³• -> è¯­ä¹‰ -> é€»è¾‘ï¼‰
        self.blocks = nn.ModuleList([
            Block(d_model, n_head) for _ in range(n_layer)
        ])

        # === 3. é›¶ä»¶å‡†å¤‡ï¼šå‡ºå£ ===
        # Final LayerNorm: ç»è¿‡å‡ åå±‚è®¡ç®—ï¼Œæ•°æ®åˆ†å¸ƒå¯èƒ½ä¹±äº†ï¼Œæœ€åæ•´ç†ä¸€ä¸‹
        self.ln_f = nn.LayerNorm(d_model)

        # LM Head (Language Model Head):
        # æŠŠéšè—å±‚ç»´åº¦ (d_model) æ˜ å°„å› è¯è¡¨ç»´åº¦ (vocab_size)
        # è¿™æ ·æ‰èƒ½çŸ¥é“ä¸‹ä¸€ä¸ªè¯æ˜¯ "def" è¿˜æ˜¯ "import" çš„æ¦‚ç‡æœ€å¤§
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

        # === ğŸŒŸ ä¸“å®¶ç‚¹æ‹¨ï¼šWeight Tying (æƒé‡å…±äº«) ===
        # è¿™æ˜¯ä¸€ä¸ªç®—æ³•æŠ€å·§ã€‚
        # ç›´è§‰ï¼šToken "def" è¿›å…¥æ¨¡å‹æ—¶ç”¨çš„å‘é‡ï¼Œå’Œæ¨¡å‹æƒ³è¾“å‡º "def" æ—¶ç”¨çš„å‘é‡ï¼Œ
        # åœ¨è¯­ä¹‰ç©ºé—´é‡Œåº”è¯¥æ˜¯ç›¸ä¼¼çš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¼ºåˆ¶è®©å®ƒä»¬å…±äº«å‚æ•°ã€‚
        # å¥½å¤„ï¼šçœäº†å¤§é‡æ˜¾å­˜ï¼Œè€Œä¸”é€šå¸¸æ•ˆæœæ›´å¥½ã€‚
        self.token_embedding.weight = self.lm_head.weight

    def forward(self, idx, targets=None):
        # idx: [Batch, Seq_Len] (æ¯”å¦‚ 2å¥è¯ï¼Œæ¯å¥10ä¸ªè¯)
        B , T = idx.size()

        # === Step 1: å‡†å¤‡â€œçœ¼ç½©â€ (Mask) ===
        # æ­¤æ—¶ç”Ÿæˆä¸€ä¸ª T x T çš„ä¸‹ä¸‰è§’çŸ©é˜µã€‚
        # ä¸ºä»€ä¹ˆåœ¨è¿™é‡Œç”Ÿæˆï¼Ÿå› ä¸ºåºåˆ—é•¿åº¦ T æ˜¯åŠ¨æ€çš„ï¼Œå¯èƒ½è¿™æ¬¡æ˜¯10ï¼Œä¸‹æ¬¡æ˜¯20
        mask = create_causal_mask(T).to(idx.device)

        # === Step 2: èåˆä¿¡æ¯ ===
        # è¯çš„ä¿¡æ¯ + ä½ç½®çš„ä¿¡æ¯ = å®Œæ•´çš„è¾“å…¥è¡¨ç¤º
        token_emb =  self.token_embedding(idx)
        position_emb = self.position_embedding(idx)
        x = token_emb + position_emb

        # === Step 3: å±‚å±‚æç‚¼ ===
        # æ•°æ®æµç»æ¯ä¸€ä¸ª Block
        for block in self.blocks:
            # è¿™é‡Œçš„ mask ä¼ è¿›å»ï¼Œå°±æ˜¯ä¸ºäº†åœ¨ Attention ç®—åˆ†æ—¶
            # æŠŠå³ä¸Šè§’ï¼ˆæœªæ¥çš„è¯ï¼‰å˜æˆ -infï¼Œè®© Softmax æ¦‚ç‡ä¸º 0
            x = block(x, mask)

        # === Step 4: æœ€ç»ˆè¾“å‡º ===
        x = self.ln_f(x)  # å½’ä¸€åŒ–
        logits = self.lm_head(x)  # [B, T, vocab_size] -> æ¯ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„â€œåˆ†æ•°â€

        # === Step 5: ç®—åˆ† (ä»…åœ¨è®­ç»ƒæ—¶) ===
        loss = None
        if targets is not None:
            # targets ä¹Ÿæ˜¯ [B, T]

            # PyTorch çš„ CrossEntropyLoss æœ‰ä¸ªæ€ªç™–ï¼š
            # å®ƒå¸Œæœ› Input æ˜¯ [æ ·æœ¬æ•°, ç±»åˆ«æ•°]ï¼Œå³ 2D çŸ©é˜µã€‚
            # ä½†æˆ‘ä»¬çš„ logits æ˜¯ 3D çš„ [Batch, Time, Vocab]ã€‚

            # æ‰€ä»¥æˆ‘ä»¬éœ€è¦æŠŠ Batch å’Œ Time æåœ¨ä¸€èµ·ï¼Œå˜æˆä¸€ä¸ªâ€œè¶…çº§é•¿â€çš„åºåˆ—ã€‚
            # view(-1, ...) çš„æ„æ€æ˜¯ï¼šæŠŠå‰ä¸¤ä¸ªç»´åº¦åˆå¹¶ï¼Œå‰©ä¸‹çš„è‡ªåŠ¨è®¡ç®—ã€‚

            # logits å˜èº«: [B*T, vocab_size]
            B_T_logits = logits.view(-1, logits.size(-1))

            # targets å˜èº«: [B*T]
            B_T_targets = targets.view(-1)

            # è¿™æ ·å°±æ˜¯æ ‡å‡†çš„åˆ†ç±»é—®é¢˜äº†ï¼š
            # å¯¹è¿™ B*T ä¸ªä½ç½®ï¼Œç®—å‡ºé¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®è·ã€‚
            loss = F.cross_entropy(B_T_logits, B_T_targets)

        return logits, loss



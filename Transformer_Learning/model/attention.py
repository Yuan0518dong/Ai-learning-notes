import torch
import torch.nn as nn
import  math
import torch.nn.functional as F


class ScaledDotProductAttention(nn.Module):
    """
    [æ‰“å·¥ä»”]
    åªè´Ÿè´£çº¯æ•°å­¦è¿ç®—ã€‚
    è¾“å…¥: å·²ç»æ˜¯åˆ†å¥½å¤´çš„ q, k, v
    è¾“å‡º: åŠ æƒåçš„ values, attention weights
    """

    def __init__(self, dropout=0.1):
        super(ScaledDotProductAttention, self).__init__()
        self.dropout = nn.Dropout(dropout)

    def forward(self, q, k, v, mask=None):
        # q, k, v ç»´åº¦: [batch, n_heads, seq_len, d_k]

        d_k = q.size(-1)

        # --- æ•°å­¦å…¬å¼æ­¥éª¤ ---

        # 1. Q * K.T / sqrt(d_k)
        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)

        # 2. Mask (å¦‚æœæœ‰)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # 3. Softmax
        attn_weights = F.softmax(scores, dim=-1)

        # 4. Dropout
        attn_weights = self.dropout(attn_weights)

        # 5. Weight * V
        output = torch.matmul(attn_weights, v)

        return output, attn_weights


class MultiHeadAttention(nn.Module):
    """
    [åŒ…å·¥å¤´]
    è´Ÿè´£ï¼šæŠ•å½±(Linear) -> åˆ†å¤´(Split) -> æŒ‡æŒ¥æ‰“å·¥ä»”è®¡ç®— -> æ‹¼æ¥(Concat) -> è¾“å‡º
    """
    def __init__(self, d_model, n_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        assert d_model % n_heads == 0, "d_model å¿…é¡»èƒ½è¢« n_heads æ•´é™¤"

        # å®šä¹‰4ä¸ªçº¿æ€§å±‚: Q, K, V çš„æŠ•å½±ï¼Œä»¥åŠæœ€åçš„è¾“å‡ºæŠ•å½±
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.fc = nn.Linear(d_model, d_model)

        # é›‡ä½£æ‰“å·¥ä»”
        self.attention = ScaledDotProductAttention(dropout=dropout)

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)

        # 1. [æŠ•å½±] æŠŠè¾“å…¥å˜ä¸ªèº«
        query = self.w_q(query)
        key   = self.w_k(key)
        value = self.w_v(value)

        # 2. [åˆ†å¤´] æœ€éš¾çš„ä¸€æ­¥ï¼šSplit & Transpose
        # view: æŠŠ d_model æ‹†æˆ n_heads * d_k
        # transpose: æŠŠ n_heads æ”¾åˆ°å‰é¢ï¼Œæ–¹ä¾¿å¹¶è¡Œè®¡ç®—
        # å½¢çŠ¶å˜åŒ–: [batch, seq, d_model] -> [batch, n_heads, seq, d_k]
        query = query.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        key   = key.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        value = value.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)

        # 3. [è®¡ç®—] äº¤ç»™æ‰“å·¥ä»”å¤„ç†
        # è¿™é‡Œçš„å‚æ•°å¿…é¡»æ˜¯3ä¸ªåˆ†å¼€çš„ q, k, v
        out, attn = self.attention(query, key, value, mask=mask)

        # 4. [æ‹¼æ¥] Concat
        # transpose: æŠŠ n_heads æ¢å›æ¥ -> [batch, seq, n_heads, d_k]
        # contiguous: å†…å­˜æ•´ç† (å¿…é¡»åšï¼Œå¦åˆ™ view æŠ¥é”™)
        # view: æ‹¼å›åŸæ¥çš„å½¢çŠ¶ -> [batch, seq, d_model]
        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # 5. [æ”¶å°¾] æœ€åçš„çº¿æ€§å˜æ¢
        out = self.fc(out)

        return out

# ==============================================================================
# æµ‹è¯•ä»£ç 
# ==============================================================================
if __name__ == "__main__":
    print("ğŸ”¥ æ­£åœ¨æµ‹è¯• MultiHeadAttention æ¨¡å—...")

    # 1. è®¾å®šå‚æ•°
    d_model = 512
    n_heads = 8
    seq_len = 10
    batch_size = 2

    # 2. å®ä¾‹åŒ–æ¨¡å—
    # æ³¨æ„ï¼šç°åœ¨ä¸éœ€è¦åœ¨è¿™é‡Œæµ‹è¯• SelfAttention äº†ï¼Œå› ä¸ºå®ƒæ˜¯ MultiHeadAttention çš„å†…éƒ¨ç»„ä»¶
    mha = MultiHeadAttention(d_model, n_heads)
    print("âœ… æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")

    # 3. åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    # å½¢çŠ¶: [batch_size, seq_len, d_model]
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"è¾“å…¥ x å½¢çŠ¶: {x.shape}")

    # 4. å‰å‘ä¼ æ’­
    # åªéœ€è¦æµ‹è¿™ä¸€æ­¥ï¼Œå°±èƒ½éªŒè¯æ•´ä¸ªé“¾è·¯ï¼ˆåŒ…æ‹¬ Linear, Split, Attention, Concatï¼‰å…¨æ˜¯å¯¹çš„
    output, attn_map = mha(x, x, x)

    # 5. éªŒè¯ç»“æœ
    print(f"è¾“å‡º output å½¢çŠ¶: {output.shape}")  # æœŸæœ›: [2, 10, 512]
    print(f"Attention Map å½¢çŠ¶: {attn_map.shape}")  # æœŸæœ›: [2, 8, 10, 10]

    assert output.shape == (batch_size, seq_len, d_model), "âŒ è¾“å‡ºç»´åº¦ä¸å¯¹ï¼"
    assert attn_map.shape == (batch_size, n_heads, seq_len, seq_len), "âŒ Attentionæƒé‡ç»´åº¦ä¸å¯¹ï¼"

    print("ğŸ‰ å¤ªæ£’äº†ï¼MultiHeadAttention æµ‹è¯•å®Œç¾é€šè¿‡ï¼")